#q- 1-step eval super-large-llm @runpod 3-gpu-vram-120gb @ssh-tmux (original one exec much faster than gguf) (20 min download 220gb llm model, 10 min exec lm_eval and get acc results); eval-llm-runpod-fa4a8af-20240127125114

opt-1: git clone https://github.com/chenhaodev/llm-autoeval; cd llm-autoeval/; python create_pods.py 'mistralai/Mistral-7B-Instruct-v0.2' 'NVIDIA RTX A4500' 1 LVEA98VLXKAR5CVCFYYPIR6FG4UCJYDLN9N3H96F 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 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 6p59tg6cln

opt-2: apt-get update; apt-get install -y tmux vim git-lfs; tmux new -s ssh-download-llm; cd /workspace/; mkdir -p cache model; pip install huggingface_hub; huggingface-cli login --token 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; huggingface-cli download --resume-download xxx/xxx --local-dir /workspace/model --local-dir-use-symlinks False --cache-dir /workspace/cache; tmux new -s ssh-config; cd /workspace/; git clone https://github.com/chenhaodev/lm-evaluation-harness; cd lm-evaluation-harness/; pip install -e .; <wait @ssh-download-llm>; lm_eval --model hf --model_args pretrained=/path-to-model,parallelize=True,load_in_4bit=True --tasks ocn,aocnp,medmcqa,pubmedqa,mmlu_clinical_knowledge,mmlu_college_medicine,mmlu_professional_medicine --device cuda:0 --batch_size auto --limit 100 --use_cache ./cache/ #--tasks mmlu_flan_cot_fewshot_clinical_knowledge #task-yaml fewshot_split: validation #--num_fewshot 5

