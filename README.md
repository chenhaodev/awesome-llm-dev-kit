This repository gathers state-of-art LLM training and testing scripts / repositories.

## llm-autoeval
https://github.com/chenhaodev/llm-autoeval
It simplifies the process of evaluating LLMs using a convenient interface. Special commit: + create_pods.py, add arg for num_of_gpu. 

## lm-evaluation-harness
https://github.com/chenhaodev/lm-evaluation-harness
Unified framework to test generative language models on a large number of different evaluation tasks. Special commit: + medical knowledge db, cancer instruct db

## openai-evals
https://github.com/chenhaodev/openai-evals
Openai official evalation framework and benchmark. Special commit: + medical knowledge db, cancer instruct db. 

## llama-factory
https://github.com/chenhaodev/llm-llama-factory
One-stop interface that create fine-tune process. My special commit: + medical knowledge db, cancer instruct db for training. 

## mergekit 
https://github.com/chenhaodev/llm-mergekit
A toolkit for merging pre-trained language models. Merges can be run entirely on CPU or accelerated with as little as 8 GB of VRAM. 

